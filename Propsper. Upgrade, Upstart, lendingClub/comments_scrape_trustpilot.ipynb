{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8756d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import urllib\n",
    "import urllib.request as ur\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from urllib.request import Request, urlopen\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb2aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_scrape_new(WEBSITE, Page):\n",
    "    comments = []\n",
    "    rates = []\n",
    "    dates = []\n",
    "\n",
    "    # type in the website you want to scrape and analyze the trends over time!!!!!\n",
    "    # example WEBSITE = 'www.betfair.com'\n",
    "    \n",
    "    user_agents_list = [\n",
    "            'Mozilla/5.0 (iPad; CPU OS 13_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_16_8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',\n",
    "            'python-requests/3.10.0',\n",
    "            'python-requests/2.14.0',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36',\n",
    "            'Mozilla/5.0 (iPhone; CPU iPhone OS 14_4_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1',\n",
    "            'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18363'\n",
    "        ]\n",
    "    \n",
    "\n",
    "    for page in range(1, Page):\n",
    "        time.sleep(random.randint(0, 2))\n",
    "        if page == 1:\n",
    "            req = Request(\n",
    "                    url='https://www.trustpilot.com/review/'+ WEBSITE + '?languages=all&sort=recency',\n",
    "                    headers={'User-Agent': random.choice(user_agents_list)}\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            req = Request(\n",
    "                    url='https://www.trustpilot.com/review/'+ WEBSITE +'?languages=all&page={}&sort=recency'.format(str(page)),\n",
    "                    headers={'User-Agent': random.choice(user_agents_list)}\n",
    "                )\n",
    "            \n",
    "        content = urlopen(req).read()\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "        table_com_date = soup.findAll('div',attrs={\"class\":\"styles_reviewContent__0Q2Tg\"})\n",
    "        table_rate = soup.findAll('div',attrs={\"class\":\"styles_reviewHeader__iU9Px\"})\n",
    "\n",
    "        # scrape rates\n",
    "        for x in table_rate:\n",
    "            rates.append(x['data-service-review-rating'])\n",
    "\n",
    "        # scrape comments and dates\n",
    "        for x in table_com_date:\n",
    "\n",
    "            comments.append(x.find('p').text)\n",
    "            date = x.find('p', attrs = {\"class\": \"typography_body-m__xgxZ_ typography_appearance-default__AAY17 typography_color-black__5LYEn\"}).text[20:]\n",
    "            date = datetime.strptime(date , \"%B %d, %Y\")\n",
    "            dates.append(date)\n",
    "        \n",
    "    d = {'Review': comments, 'Rate': rates,'Date': dates }\n",
    "    df = pd.DataFrame(d)\n",
    "    df = df.set_index(['Date'])\n",
    "    df['Company'] = WEBSITE\n",
    "    \n",
    "    # print ratio\n",
    "    print('Rate counts shows below:')\n",
    "    print(df['Rate'].value_counts())\n",
    "    print('\\nRate counts ratio shows below:')\n",
    "    print(df.Rate.value_counts(normalize=True))\n",
    "    \n",
    "    print('\\nReview groupby year shows below:')\n",
    "    print(df.groupby(df.index.year)['Review'].count())\n",
    "    \n",
    "    print('\\nScraped ' + str(len(df)) + ' reviews of ' + WEBSITE)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "329095e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trends_analyze_new(filename, Rate, Date, Company):\n",
    "    \n",
    "    # read\n",
    "    df = pd.read_csv (filename, lineterminator='\\n')\n",
    "    \n",
    "    if Company == 'All':\n",
    "        df = df\n",
    "    else:\n",
    "        df = df.loc[df['Company'] == Company]\n",
    "\n",
    "    # joins all the sentenses\n",
    "    df['Review'] = [df['Review'].iloc[i].replace(\"\\r\", \" \") for i in range(len(df['Review']))]\n",
    "    df['Date'] = [datetime.strptime(df['Date'].iloc[i], \"%Y-%m-%d\") for i in range(len(df['Review']))]\n",
    "    df = df.set_index(['Date'])\n",
    "    df = df.loc[df['Rate'] == Rate].loc[Date]\n",
    "    \n",
    "    sentence = \" \".join(df['Review'])\n",
    "    # creates tokens, creates lower class, removes numbers and lemmatizes the words\n",
    "    new_tokens = word_tokenize(sentence)\n",
    "    new_tokens = [t.lower() for t in new_tokens]\n",
    "    new_tokens =[t for t in new_tokens if t not in stopwords.words('english')]\n",
    "    new_tokens = [t for t in new_tokens if t.isalpha()]\n",
    "\n",
    "    # #counts the words, pairs and trigrams\n",
    "    counted = Counter(new_tokens)\n",
    "    counted_2= Counter(ngrams(new_tokens,2))\n",
    "    counted_3= Counter(ngrams(new_tokens,3))\n",
    "\n",
    "    # #creates 3 data frames and returns thems\n",
    "    word_freq = pd.DataFrame(counted.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    word_pairs =pd.DataFrame(counted_2.items(),columns=['pairs','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    trigrams =pd.DataFrame(counted_3.items(),columns=['trigrams','frequency']).sort_values(by='frequency',ascending=False)\n",
    "\n",
    "    return word_freq, word_pairs, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f325d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart_new(word_freq, word_pairs, trigrams):\n",
    "    # create subplot of the different data frames\n",
    "    fig, axes = plt.subplots(3,1,figsize=(9,15))\n",
    "    sns.barplot(ax=axes[0],x='frequency',y='word',data=word_freq.head(30))\n",
    "    sns.barplot(ax=axes[1],x='frequency',y='pairs',data=word_pairs.head(30))\n",
    "    sns.barplot(ax=axes[2],x='frequency',y='trigrams',data=trigrams.head(30))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b09e04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_chart_plot_new(filename, Company):\n",
    "    # read\n",
    "    df = pd.read_csv (filename, lineterminator='\\n')\n",
    "    \n",
    "    # joins all the sentenses\n",
    "    df['Review'] = [df['Review'].iloc[i].replace(\"\\r\", \" \") for i in range(len(df['Review']))]\n",
    "    df['Date'] =  pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "    df = df.set_index(['Date'])\n",
    "\n",
    "    if Company == 'All':\n",
    "        df = df\n",
    "    else:\n",
    "        df = df.loc[df['Company'] == Company].drop(['Company'], axis=1)\n",
    "    \n",
    "    df['Rate'] = df['Rate'].astype(float)\n",
    "    \n",
    "    \n",
    "    # review_star_split\n",
    "    \n",
    "    fig0, axd = plt.subplot_mosaic([['ax0', 'ax0'], ['ax1', 'ax2'], ['ax3','ax3']],figsize=(15, 15))\n",
    "    df.groupby([df.index.year, \"Rate\"]).count().unstack().plot(kind='bar', edgecolor='black',ax = axd['ax0'])\n",
    "    axd['ax0'].set_title(\"Review Star Distribution Over Time of \"+ Company)\n",
    "    axd['ax0'].legend(title=\"\")\n",
    "    \n",
    "    df['Rate'].plot(kind='hist', edgecolor='black',ax=axd['ax1'])\n",
    "    axd['ax1'].set_title(\"Review Star Distribution\")\n",
    "    \n",
    "    df_rs = df.groupby([df.index.year, \"Rate\"]).count()\n",
    "    df_rs = df_rs.groupby(level=0, group_keys=False).apply(lambda x:100 * x / float(x.sum()))\n",
    "    df_rs.unstack().plot(kind='bar', stacked=True, ax=axd['ax2'])\n",
    "    \n",
    "    axd['ax2'].set_title(\"Review Star Percentage over Time\")\n",
    "    axd['ax2'].legend(title=\"\")\n",
    "    \n",
    "    # review_volume_trend\n",
    "\n",
    "    df_rv = df.groupby(df.index.year).count().pop('Review')\n",
    "    df_rv.plot(kind='bar', stacked=True, ax=axd['ax3'])\n",
    "    axd['ax3'].set_ylabel('Number of Reviews')\n",
    "    axd['ax3'].set_title(\"Review Volume Trend\")\n",
    "    \n",
    "    # review_pie_chart\n",
    "    \n",
    "    number_group = df.groupby(df.index.year).ngroups\n",
    "    \n",
    "    df_rp = df.groupby([df.index.year, \"Rate\"]).count()\n",
    "    df_rp = df_rp.groupby(level=0, group_keys=False).apply(lambda x:100 * x / float(x.sum()))\n",
    "\n",
    "    nrow = len(df_rp.index.levels[0])//4 + 1\n",
    "\n",
    "    fig2 = plt.figure(figsize=(15, 4*nrow))\n",
    "    \n",
    "    for i, e in enumerate(df_rp.index.levels[0]):\n",
    "        ax = fig2.add_subplot(nrow, 4, i+1)\n",
    "        yy = df_rp.loc[e][\"Review\"].tolist()\n",
    "        labels = list(df_rp.loc[e][\"Review\"].index.values)\n",
    "        ax.pie(yy, shadow=True, startangle=90)\n",
    "        labels = [f'{l}, {s:0.1f}%' for l, s in zip(labels, yy)]\n",
    "        plt.legend(bbox_to_anchor=(0.85, 1), loc='upper left', labels=labels)\n",
    "        ax.set_title(e)  \n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.1)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e61466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(WEBSITE):\n",
    "    filename = 'trustpilot_reviews_' + WEBSITE + '.pdf'\n",
    "\n",
    "    p = PdfPages(filename)\n",
    "    \n",
    "    fig_nums = plt.get_fignums()\n",
    "    figs = [plt.figure(n) for n in fig_nums]\n",
    "    \n",
    "    for fig in figs: \n",
    "        \n",
    "        # and saving the files\n",
    "        fig.savefig(p, format='pdf') \n",
    "\n",
    "    # close the object\n",
    "    p.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Company (1)\n",
    "\n",
    "WEBSITE = 'lendingclub.com'\n",
    "Page = 229\n",
    "df = comment_scrape_new(WEBSITE, Page)\n",
    "df.to_csv('review_trustpilot_{}.csv'.format(WEBSITE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Company (2)\n",
    "\n",
    "filename = 'review_trustpilot_LendingClub.csv'\n",
    "Company = 'LendingClub'\n",
    "review_chart_plot_new(filename, Company)\n",
    "save_image(Company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "276c34f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m df_bag \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(WEBSITE)):\n\u001b[0;32m----> 8\u001b[0m     df \u001b[39m=\u001b[39m comment_scrape_new(WEBSITE[i], Page[i])\n\u001b[1;32m      9\u001b[0m     df_bag \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_bag, df])\n",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m, in \u001b[0;36mcomment_scrape_new\u001b[0;34m(WEBSITE, Page)\u001b[0m\n\u001b[1;32m     32\u001b[0m     req \u001b[39m=\u001b[39m Request(\n\u001b[1;32m     33\u001b[0m             url\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://www.trustpilot.com/review/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m WEBSITE \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m?languages=all&page=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m&sort=recency\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mstr\u001b[39m(page)),\n\u001b[1;32m     34\u001b[0m             headers\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: random\u001b[39m.\u001b[39mchoice(user_agents_list)}\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     37\u001b[0m content \u001b[39m=\u001b[39m urlopen(req)\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 38\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(content, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m table_com_date \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfindAll(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m,attrs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mstyles_reviewContent__0Q2Tg\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m     41\u001b[0m table_rate \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfindAll(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m,attrs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mstyles_reviewHeader__iU9Px\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/bs4/__init__.py:248\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    247\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    249\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    253\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "WEBSITE = ['lendingclub.com', 'www.prosper.com', 'upgrade.com', 'www.upstart.com', 'avant.com']\n",
    "Page = [229, 542, 1856, 2016, 1156]\n",
    "\n",
    "df_bag = pd.DataFrame()\n",
    "\n",
    "for i in range(len(WEBSITE)):\n",
    "    \n",
    "    df = comment_scrape_new(WEBSITE[i], Page[i])\n",
    "    df_bag = pd.concat([df_bag, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bag.to_csv('review_trustpilot.csv')\n",
    "\n",
    "filename = 'review_trustpilot.csv'\n",
    "\n",
    "for i in WEBSITE:\n",
    "    review_chart_plot_new(filename, i)\n",
    "    save_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8badfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
